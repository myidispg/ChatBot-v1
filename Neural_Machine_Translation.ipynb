{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myidispg/NLP-Projects/blob/master/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8MDJXskZOXRe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# My first Google Colab notebook for Neural Machine Translation in Pytorch.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WDPUkqq_oNlz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **The necessary imports and running the code on GPU if available.**\n",
        "GPU is necessary for faster model training."
      ]
    },
    {
      "metadata": {
        "id": "-ihN_CB6Nvcd",
        "colab_type": "code",
        "outputId": "2b75090f-8fe3-4f0c-d551-3f5b05b5d24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import tarfile\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "JBvdiuH4rSjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Download and save the dataset"
      ]
    },
    {
      "metadata": {
        "id": "iYvYgfnGojvi",
        "colab_type": "code",
        "outputId": "8d644819-0399-4fc5-f4da-9ab06ea037a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dataset_url = 'http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.tgz'\n",
        "tgz_file = requests.get(dataset_url, stream=True)\n",
        "\n",
        "with open(\"parallel.tgz\", \"wb\") as f:\n",
        "  for chunk in tgz_file.iter_content(chunk_size=1024):\n",
        "    if chunk:\n",
        "      f.write(chunk)\n",
        "      \n",
        "if path.exists(\"parallel.tgz\"):\n",
        "  print('File saved successfully.')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File saved successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3RpEwje4rW5v",
        "colab_type": "code",
        "outputId": "f3846cb2-c116-43b7-8144-85f243fd158d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "\n",
        "tar = tarfile.open(\"parallel.tgz\")\n",
        "for member in tar.getmembers():\n",
        "  f = tar.extractfile(member)\n",
        "  if f is not None:\n",
        "    data_list.append(f)\n",
        "\n",
        "data_list\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<ExFileObject name='parallel.tgz'>, <ExFileObject name='parallel.tgz'>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "THtBuDiYsoTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a class for a Language\n",
        "This class will contain the word2idx, idx2word, number of words in the vocabulary and max sentence length of that language."
      ]
    },
    {
      "metadata": {
        "id": "DN92m4znsnVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2\n",
        "        self.max_sent_length = 1\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        sent_length = len(sentence.split(' '))\n",
        "        self.max_sent_length = sent_length if sent_length > self.max_sent_length else self.max_sent_length        \n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "    \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "            \n",
        "hindi_lang = Lang('hindi')\n",
        "english_lang = Lang('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xmm5MrNftCyW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Some helper functions to read the data, create pairs and the language vocabularies."
      ]
    },
    {
      "metadata": {
        "id": "uankOoTls5bq",
        "colab_type": "code",
        "outputId": "e1a3b5f8-f14c-49ad-f854-21348e3beb16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "def addWordsToLang(lang, lines):\n",
        "    for line in lines:\n",
        "        lang.addSentence(line)\n",
        "    \n",
        "    return lang\n",
        "\n",
        "def create_pairs(lang1, lang2):\n",
        "    pairs = []\n",
        "\n",
        "    for lang1_sent, lang2_sent in zip(lang1, lang2):\n",
        "        pairs.append([lang1_sent, lang2_sent])\n",
        "        \n",
        "    return pairs\n",
        "\n",
        "def createLanguagesAndPairs(data_list, lang1, lang2):\n",
        "    print('Opening files and reading the sentences')\n",
        "      \n",
        "    lang1_lines = data_list[0].read().strip().split('\\n')\n",
        "    lang2_lines = data_list[1].read().strip().split('\\n')\n",
        "    \n",
        "    print('Creating pairs...')\n",
        "    pairs = create_pairs(lang1_lines, lang2_lines)\n",
        "    \n",
        "    print('Adding words to languages')\n",
        "    lang1 = addWordsToLang(lang1, lang1_lines)\n",
        "    lang2 = addWordsToLang(lang2, lang2_lines)\n",
        "    \n",
        "    print('Done creating languages')\n",
        "    \n",
        "    return pairs, lang1, lang2\n",
        "  \n",
        "pairs, hindi_lang, english_lang = createLanguagesAndPairs(data_list, hindi_lang, english_lang)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening files and reading the sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8842a5a5e05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindi_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateLanguagesAndPairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhindi_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-8842a5a5e05f>\u001b[0m in \u001b[0;36mcreateLanguagesAndPairs\u001b[0;34m(data_list, lang1, lang2)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Opening files and reading the sentences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlang1_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mlang2_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
          ]
        }
      ]
    }
  ]
}