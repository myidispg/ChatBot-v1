{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation English-Hindi.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myidispg/NLP-Projects/blob/master/Neural_Machine_Translation_English_Hindi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8MDJXskZOXRe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# My first Google Colab notebook for Neural Machine Translation in Pytorch.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WDPUkqq_oNlz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **The necessary imports and running the code on GPU if available.**\n",
        "GPU is necessary for faster model training."
      ]
    },
    {
      "metadata": {
        "id": "-ihN_CB6Nvcd",
        "colab_type": "code",
        "outputId": "cd1d804c-9f30-437d-f6ac-c08bed557746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "device"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "JBvdiuH4rSjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Download and save the dataset"
      ]
    },
    {
      "metadata": {
        "id": "iYvYgfnGojvi",
        "colab_type": "code",
        "outputId": "a7075715-7b15-4739-9038-0c556c7f521b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "dataset_url = 'http://www.cfilt.iitb.ac.in/iitb_parallel/iitb_corpus_download/parallel.tgz'\n",
        "tgz_file = requests.get(dataset_url, stream=True)\n",
        "\n",
        "if path.exists(\"parallel.tgz\"):\n",
        "  os.remove('parallel.tgz')\n",
        "  print('Removed the existing copy')\n",
        "\n",
        "with open(\"parallel.tgz\", \"wb\") as f:\n",
        "  for chunk in tgz_file.iter_content(chunk_size=1024):\n",
        "    if chunk:\n",
        "      f.write(chunk)\n",
        "      \n",
        "if path.exists(\"parallel.tgz\"):\n",
        "  print('File saved successfully.')\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed the existing copy\n",
            "File saved successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqNB2hsCdRgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b9ae391-6d0d-4824-e63e-6903b8c28547"
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "urlretrieve(dataset_url, 'parallel.tgz')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('parallel.tgz', <http.client.HTTPMessage at 0x7fa38851c828>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "3RpEwje4rW5v",
        "colab_type": "code",
        "outputId": "fe720184-f54f-4b9c-cbf6-b4dc36590ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "\n",
        "tar = tarfile.open(\"parallel.tgz\")\n",
        "tarinfo = tarfile.TarInfo(tar)\n",
        "for member in tar.getmembers():\n",
        "  print(member)\n",
        "  f = tar.extractfile(member)\n",
        "  print(f)\n",
        "  if f is not None:\n",
        "    data_list.append(f)\n",
        "\n",
        "data_list\n",
        "  "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<TarInfo 'parallel' at 0x7fa389d7b4f8>, <TarInfo 'parallel/IITB.en-hi.hi' at 0x7fa389d7b818>, <TarInfo 'parallel/IITB.en-hi.en' at 0x7fa389d7b750>]\n",
            "<TarInfo 'parallel' at 0x7fa389d7b4f8>\n",
            "None\n",
            "<TarInfo 'parallel/IITB.en-hi.hi' at 0x7fa389d7b818>\n",
            "<ExFileObject name='parallel.tgz'>\n",
            "<TarInfo 'parallel/IITB.en-hi.en' at 0x7fa389d7b750>\n",
            "<ExFileObject name='parallel.tgz'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<ExFileObject name='parallel.tgz'>, <ExFileObject name='parallel.tgz'>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "THtBuDiYsoTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a class for a Language\n",
        "This class will contain the word2idx, idx2word, number of words in the vocabulary and max sentence length of that language."
      ]
    },
    {
      "metadata": {
        "id": "DN92m4znsnVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2\n",
        "        self.max_sent_length = 1\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        sent_length = len(sentence.split(' '))\n",
        "        self.max_sent_length = sent_length if sent_length > self.max_sent_length else self.max_sent_length        \n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "    \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "            \n",
        "hindi_lang = Lang('hindi')\n",
        "english_lang = Lang('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nX9TPlKJSU2G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use the .tgz file and read the ExFileObject to get the list of lines and read in utf-8 format"
      ]
    },
    {
      "metadata": {
        "id": "1ilexu_CN-7N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bytecode_lines = data_list[1].readlines()\n",
        "\n",
        "english_lines = []\n",
        "\n",
        "for line in bytecode_lines:\n",
        "  english_lines.append(line.decode('utf-8').strip('\\n'))\n",
        "  \n",
        "hindi_lines = []\n",
        "bytecode_lines = data_list[0].readlines()\n",
        "\n",
        "for line in bytecode_lines:\n",
        "  hindi_lines.append(line.decode('utf-8').strip('\\n'))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xmm5MrNftCyW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Some helper functions to read the data, create pairs and the language vocabularies."
      ]
    },
    {
      "metadata": {
        "id": "uankOoTls5bq",
        "colab_type": "code",
        "outputId": "41351b35-7e5a-4946-ea14-d40764b5155a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "def addWordsToLang(lang, lines):\n",
        "    for line in lines:\n",
        "        lang.addSentence(line)\n",
        "    \n",
        "    return lang\n",
        "\n",
        "def create_pairs(lang1, lang2):\n",
        "    pairs = []\n",
        "\n",
        "    for lang1_sent, lang2_sent in zip(lang1, lang2):\n",
        "        pairs.append([lang1_sent, lang2_sent])\n",
        "        \n",
        "    return pairs\n",
        "\n",
        "def createLanguagesAndPairs(lang1_lines, lang2_lines, lang1, lang2):\n",
        "    \n",
        "    print('Creating pairs...')\n",
        "    pairs = create_pairs(lang1_lines, lang2_lines)\n",
        "    \n",
        "    print('Adding words to languages')\n",
        "    lang1 = addWordsToLang(lang1, lang1_lines)\n",
        "    lang2 = addWordsToLang(lang2, lang2_lines)\n",
        "    \n",
        "    print('Done creating languages')\n",
        "    \n",
        "    return pairs, lang1, lang2\n",
        "  \n",
        "pairs, hindi_lang, english_lang = createLanguagesAndPairs(hindi_lines, english_lines, hindi_lang, english_lang)\n",
        "\n",
        "MAX_LENGTH = english_lang.max_sent_length if english_lang.max_sent_length > hindi_lang.max_sent_length else hindi_lang.max_sent_length\n",
        "print(f'No of words in english: {english_lang.n_words}, No of words in hindi: {hindi_lang.n_words}, Max length of sentence in both: {MAX_LENGTH}')\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating pairs...\n",
            "Adding words to languages\n",
            "Done creating languages\n",
            "No of words in english: 462621, No of words in hindi: 536013, Max length of sentence in both: 1917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r1CyMlN3Sqvl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Encoder RNN\n",
        "Create an Encoder RNN. \n",
        "It takes the input size which is the number of words in the input language vocabulary.\n",
        "The other argument is the hidden state dimension. The dimensions of the embedidng is also the same as the hidden state dimensions.\n",
        "\n",
        "\n",
        "\n",
        "![The Encoder RNN Image](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ]
    },
    {
      "metadata": {
        "id": "4R5273ecSqc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    \n",
        "  def forward(self, input, hidden_state):\n",
        "    embedded = self.embedding(input).view(1, 1, -1)\n",
        "    output = embedded\n",
        "    output, hidden_state = self.gru(output, hidden_state)\n",
        "    return output, hidden_state\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.randn(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7N2skMGdUOL-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create Decoder RNN\n",
        "Create the DecoderRNN. It takes the hidden unit dimensions and the number of words in the output language vocabulary.\n",
        "\n",
        "\n",
        "![DecoderRNN architecture](https://pytorch.org/tutorials/_images/decoder-network.png)"
      ]
    },
    {
      "metadata": {
        "id": "R-ya0R_BUn4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, input, hidden_state):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden_state)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden_state\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.randn(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ly5NOpzMUuud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training section.\n",
        "Some functions to create a sequence of inputs for each sentence pair.\n"
      ]
    },
    {
      "metadata": {
        "id": "eOX6A3jvU3I0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "    \n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorFromPairs(pair):\n",
        "    input_tensor = tensorFromSentence(hindi_lang, pair[0])\n",
        "    output_tensor = tensorFromSentence(english_lang, pair[1])\n",
        "    return (input_tensor, output_tensor)\n",
        "\n",
        "# This section is for testing the outputs of the Encoder\n",
        "input_tensor, output_tensor = tensorFromPairs(pairs[0])\n",
        "\n",
        "HIDDEN_DIM = 256\n",
        "encoder = EncoderRNN(english_lang.n_words, HIDDEN_DIM).to(device)\n",
        "decoder = DecoderRNN(HIDDEN_DIM, hindi_lang.n_words).to(device)\n",
        "\n",
        "encoder_hidden = encoder.initHidden()\n",
        "\n",
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n",
        "\n",
        "encoder_optimizer.zero_grad()\n",
        "decoder_optimizer.zero_grad()\n",
        "\n",
        "encoder_output, encoder_hidden = encoder(input_tensor[0], encoder_hidden)\n",
        "\n",
        "print(f'encoder_output- \\n{encoder_output}\\nencoder_hidden- {encoder_hidden}\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhi4kGkUEfyW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Teacher forcing** is the concept of using the real target outputs as each next input,  instead of using the decoderâ€™s guess as the next input. Using teacher forcing  causes it to converge faster but when the trained network is exploited, it may exhibit instability."
      ]
    },
    {
      "metadata": {
        "id": "UjO2-7s-Em46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
        "  encoder_hidden = encoder.initHidden()\n",
        "  \n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  input_length = input_tensor.shape[0]\n",
        "  output_length = output_tensor.shape[0]\n",
        "  \n",
        "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device-device)\n",
        "  \n",
        "  loss = 0\n",
        "  \n",
        "  for ei in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input, encoder_hidden)\n",
        "    encoder_outputs[ei] = encoder_output[0, 0]\n",
        "    \n",
        "  decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
        "  \n",
        "  decoder_hidden = encoder_hidden\n",
        "  \n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "  \n",
        "  if use_teacher_forcing:\n",
        "    for di in range(output_length):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      \n",
        "      loss += criterion(decoder_output, output_tensor[di])\n",
        "      decoder_input = output_tensor[di]\n",
        "      \n",
        "  else:\n",
        "    for di in range(output_length):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "      loss += criterion(decoder_output, outptu_tensor[di])\n",
        "      if decoder_input.item() == EOS_TOKEN:\n",
        "        break\n",
        "        \n",
        "  loss.backward()\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "  \n",
        "  return loss.item() / output_length\n",
        " \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tfHsDWUPCix",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Some functions to find time elapsed\n",
        "These functions help to calculate the elapsed time and the remaining time.\n"
      ]
    },
    {
      "metadata": {
        "id": "f-fidD-XPLI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFiwZGqXPPfL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### This function goes through all the pairs and calls the train() function."
      ]
    },
    {
      "metadata": {
        "id": "CulLehO5PZKo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0\n",
        "  plot_loss_total = 0\n",
        "  \n",
        "  training_pairs = [tensorsFromPairs(random.choice(pairs)) for i in range(n_iters)]\n",
        "  print(f'The number of training_pairs is {len(training_pairs)}\\n\\n\\n')\n",
        "  \n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "  \n",
        "  for iter in range(1, n_iters+1):\n",
        "    training_pair = training_pairs[iter-1]\n",
        "    input_tensor = training_pair[0]\n",
        "    output_tensor = training_pair[1]\n",
        "    \n",
        "    loss = train(input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH)\n",
        "    \n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "    \n",
        "    if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}